# VLM training for long context video caption

## О проекте

Данные проект предназначен для обучения моделей VLM для генерации описания видео (video captioning) на основе длинного контекста.

## Данные

Большинство существующих наборов данных и benchmark, предназначенных для задач Video-to-Text, основываются на небольших видеозаписях (15 сек. - 1 min) и текстовых подсказках к ним.

Поэтому для обучения модели для оценки видео с длинным контекстом (порядка 25 минут) был собран набор данных, основанный 
на первых сериях аниме и их текстовых описаниях. Использование домена с аниме позволяет воспользоваться огромным количеством 
произведений для сбора высококачественных данных без дополнетельной разметки.

Описание способа сбора данных и формат набора данных представлены в описании [модуля по сбору данных](src_dataset_creator).

Собранный набор данных из 614 произведений можно загрузить [здесь](https://drive.google.com/file/d/1ujlUoBU4QYXx06d_Yp_zEnpx_VaxlTFp/view?usp=sharing).

## Окружение

Для запуска обучения необходимо настроить окружение среды следующим образом.

1. Установить Pytorch
```shell
pip install torch --index-url https://download.pytorch.org/whl/cu128
```
2. Установить библиотеки
```shell
pip install -r requirements.txt
```
3. Установить flash attention

- Linux
```shell
pip install flash-attn --no-build-isolation
```
- Windows

Загрузить собранный образ из https://github.com/Dao-AILab/flash-attention/releases 
(к прим. `flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSE-cp312-cp312-win_amd64.whl`) и установить следующей командой
```shell
pip install --no-dependencies --upgrade .\flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSE-cp312-cp312-win_amd64.whl
```

Окружение для сбора набора данных описано [здесь](src_dataset_creator/README.md). 


## Обучение

Для обучения на собранном наборе данных была использована модель [SmolVLM2](https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct).

Для уменьшение размера требуемой памяти модель оборачивается с помощью LoRA адаптера. 

Перед обучением данные подготавливаются в режим чата и задачей модели является предсказание следующего токена чата (за исключением токенов изображений).

Для обучения модели используется `trl.SFTTrainer`, но результат его работы не отличается от использования `transformers.Trainer`.

Более подробно с кодом обучения можно ознакомиться в [тетрадки](notebooks/train_vlm_for_anime_caption.ipynb).

В процессе обучения остались нерешенными следующие проблемы:
- Применение обучения на основе только ответа асистента на текущий момент штатно не реализуема в связи с отсутствием специальных маркеров `{% generate %}` 
и ошибок в формировании маски в текущей реализации apply_chat_template в библиотеки `transformers`.
- Использование квантование модели для QLoRA приводит к автоматической конвертации входных данных во float32. Из-за чего при использовании модели в bfloat16 возникают ошибки несопоставимости типов данных между bfloat16 и float32 (особенности реализации SmolVLM). А без конвертации модели в bloat16 теряется возможность использование `flash-attention`.
- Даже при обучении модели на 1 батче с использованием 24 входных кадров заполнение видеопамяти достигает 10 GB. 
В следствие этого на текущем оборудовании (RTX 4060 Laptop) одна эпоха обучения занимает 1 час и 99% времени GPU простаивает без нагрузки.

## Выводы

В процессе работы были достигнуты следующие важные результаты:
- Создан скрипт по созданию набора данных
- Собран набор данных для задачи Video Caption на основе длинного контекста и загружен в открытый доступ
- Реализован полноценный работоспособный код обучения VLM модели на собранном наборе данных

К сожалению, в процессе обучения не удалось получить валидные веса модели ввиду ограниченности оборудования. 
Но собранный набор данных может быть использован другими исследователями для обучения больших VLM как новая качественная доменная область.