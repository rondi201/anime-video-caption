{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-31T00:27:25.187163Z",
     "start_time": "2025-07-31T00:27:20.147813Z"
    }
   },
   "source": [
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, ProcessorMixin, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим параметры модели, набора данных, обработчика и peft адаптеров",
   "id": "3fc7d82466e58be2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:27:25.196495Z",
     "start_time": "2025-07-31T00:27:25.192496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Путь до набора данных\n",
    "dataset_path = \"../src_dataset_creator/dataset/AniDataset_t614\"\n",
    "# Модель для обучения с HuggingFace\n",
    "model_id = \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\"\n",
    "# Зададим устройство запуска как \"auto\" - huggingface обертка выберет доступное устройство автоматически\n",
    "device = \"auto\"\n",
    "# Выберем реализацию flash_attention\n",
    "attn_implementation = \"flash_attention_2\" # sdpa, flex_attention, flash_attention_2\n",
    "# Выберем желаемую точность вычислений\n",
    "torch_dtype = torch.bfloat16\n",
    "# Использовать ли LoRA для обучения модели\n",
    "use_lora = True\n",
    "use_qlora = False\n",
    "# Максимальное количество кадров с видео (понизим с 64 до 32 для уменьшения занимаемого объема памяти)\n",
    "max_frames = 32"
   ],
   "id": "c70304f545d4e7e9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:27:32.810358Z",
     "start_time": "2025-07-31T00:27:32.806187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Подготовим конфиг LoRA\n",
    "lora_config = None\n",
    "bnb_config = None\n",
    "if use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "        use_dora=True,\n",
    "        inference_mode=False,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "if use_qlora:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )"
   ],
   "id": "6c7e653bd044896",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:27:33.459549Z",
     "start_time": "2025-07-31T00:27:33.455550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ],
   "id": "fac6fb9d56b05ec0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузим небольшую модель VLM",
   "id": "4401b94c81b9f30d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:27:51.945018Z",
     "start_time": "2025-07-31T00:27:47.705986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузим модель\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    ")\n",
    "if lora_config is not None:\n",
    "    # Применим LoRA к модели\n",
    "    if use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    # model = get_peft_model(model, lora_config)\n",
    "    # Альтернативный способ применения Lora\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "\n",
    "# Выведем информацию о модели\n",
    "print_trainable_parameters(model)\n",
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")"
   ],
   "id": "dc5d97835a9691db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 507482304 || all params: 507482304 || trainable%: 100.00\n",
      "The model as is is holding: 0.95 of GPU RAM\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Подготовим набор данных",
   "id": "6912399bd761d324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузим обработчик данных на входе/выходе модели",
   "id": "21af37742da6e452"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:52:16.625897Z",
     "start_time": "2025-07-30T23:52:15.363893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor: ProcessorMixin = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "# Изменим количество кадров\n",
    "if max_frames:\n",
    "    processor.video_processor.num_frames = max_frames"
   ],
   "id": "b7c0d39d13e5bd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим класс набора данных",
   "id": "fda4c8e2d29cbda2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:52:45.755434Z",
     "start_time": "2025-07-30T23:52:45.750195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnimeData:\n",
    "    id: str\n",
    "    mal_id: str\n",
    "    name: str\n",
    "    title: str\n",
    "    rating: str\n",
    "    score: float\n",
    "    released: datetime.date\n",
    "    genres: list[str]\n",
    "    main_characters: list[str]\n",
    "    popularity: int\n",
    "    description: str\n",
    "    video_path: str\n",
    "\n",
    "    def to_json(self) -> dict[str, Any]:\n",
    "        data = asdict(self)\n",
    "        data['released'] = data['released'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, data: dict[str, Any]) -> \"AnimeData\":\n",
    "        data['released'] = datetime.strptime(data['released'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        return cls(**data)"
   ],
   "id": "7435fa80bd72c114",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:52:46.131146Z",
     "start_time": "2025-07-30T23:52:46.123634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnimeEpisodeCaptionDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_path: str | Path\n",
    "    ):\n",
    "        dataset_path = Path(dataset_path)\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        # Загрузим аннотацию\n",
    "        annotation_path = dataset_path / \"annotation.json\"\n",
    "        with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            anime_dataset = json.load(f)\n",
    "        # Преобразуем информацию об элементе в класс данных\n",
    "        anime_data: list[AnimeData] = [\n",
    "            AnimeData.from_json(data)\n",
    "            for data in anime_dataset['animes']\n",
    "        ]\n",
    "        # Проверим валидность данных\n",
    "        for data in anime_data:\n",
    "            self._validate_anime_data(data)\n",
    "\n",
    "        self.anime_data = anime_data\n",
    "\n",
    "    def _validate_anime_data(self, data: AnimeData):\n",
    "        \"\"\" Проверка валидности данных об аниме \"\"\"\n",
    "        # Проверим, что видео доступно\n",
    "        if not (video_path := Path(self.dataset_path, data.video_path)).exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"No found video file at '{video_path}' for '{data.name}' title with id {data.mal_id}\"\n",
    "            )\n",
    "        # Проверим, что описание не пустое\n",
    "        if not data.description:\n",
    "            raise ValueError(\n",
    "                f\"No found description for '{data.name}' title with id {data.mal_id}\"\n",
    "            )\n",
    "\n",
    "    def get_anime_data_by_idx(self, item) -> AnimeData:\n",
    "        return self.anime_data[item]\n",
    "\n",
    "    def __getitem__(self, item) -> dict[str, list[dict[str, Any]]]:\n",
    "        anime_data = self.anime_data[item]\n",
    "\n",
    "        user_content = [\n",
    "            {\"type\": \"text\", \"text\": \"Caption the video. \"},\n",
    "            {\"type\": \"video\", \"path\": str(Path(self.dataset_path, anime_data.video_path))}\n",
    "        ]\n",
    "        if anime_data.main_characters:\n",
    "            mc_info = ', '.join(anime_data.main_characters)\n",
    "            user_content.insert(\n",
    "                0,\n",
    "                {\"type\": \"text\", \"text\": f\"The main characters are {mc_info}. \"}\n",
    "            )\n",
    "        if anime_data.genres:\n",
    "            g_info = ', '.join(anime_data.genres)\n",
    "            user_content.insert(\n",
    "                0,\n",
    "                {\"type\": \"text\", \"text\": f\"The video genres are {g_info}. \"}\n",
    "            )\n",
    "        assistant_content = [\n",
    "            {\"type\": \"text\", \"text\": anime_data.description}\n",
    "        ]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anime_data)\n"
   ],
   "id": "ef57d209085e1a73",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим экземпляр класса данных",
   "id": "e428d7841c8be700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:52:48.981729Z",
     "start_time": "2025-07-30T23:52:48.951387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anime_dataset = AnimeEpisodeCaptionDataset(\n",
    "    dataset_path=dataset_path,\n",
    ")\n",
    "print(f'Successfully load {len(anime_dataset)} anime data')"
   ],
   "id": "e41944d66c6c4dc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load 614 anime data\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим пример выходных данных",
   "id": "cd0facce06c12c14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:45:20.921568Z",
     "start_time": "2025-07-30T23:45:20.916085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_message = anime_dataset[0]['messages']\n",
    "example_message"
   ],
   "id": "6cfc4c38cc4b11cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'The video genres are Shounen, Action, School, Super Power. '},\n",
       "   {'type': 'text',\n",
       "    'text': 'The main characters are All Might, Katsuki Bakugou, Tenya Iida, Izuku Midoriya, Ochako Uraraka. '},\n",
       "   {'type': 'text', 'text': 'Caption the video. '},\n",
       "   {'type': 'video',\n",
       "    'path': '..\\\\src_dataset_creator\\\\dataset\\\\AniDataset_t614\\\\videos\\\\31964\\\\Boku no Hero Academia_S1_E1_720.mp4'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'The appearance of \"quirks,\" newly discovered super powers, has been steadily increasing over the years, with 80 percent of humanity possessing various abilities from manipulation of elements to shapeshifting. This leaves the remainder of the world completely powerless, and Izuku Midoriya is one such individual.\\n\\nSince he was a child, the ambitious middle schooler has wanted nothing more than to be a hero. Izuku\\'s unfair fate leaves him admiring heroes and taking notes on them whenever he can. But it seems that his persistence has borne some fruit: Izuku meets the number one hero and his personal idol, All Might. All Might\\'s quirk is a unique ability that can be inherited, and he has chosen Izuku to be his successor!\\n\\nEnduring many months of grueling training, Izuku enrolls in UA High, a prestigious high school famous for its excellent hero training program, and this year\\'s freshmen look especially promising. With his bizarre but talented classmates and the looming threat of a villainous organization, Izuku will soon learn what it really means to be a hero.'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим на входной текст после применения шаблона",
   "id": "908e776cf8430a0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:04.805730Z",
     "start_time": "2025-07-30T22:00:02.066863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_instance = processor.apply_chat_template(\n",
    "            example_message,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "processor.batch_decode(example_instance[\"input_ids\"])"
   ],
   "id": "f874c04322851bc9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have used fast image processor with LANCZOS resample which not yet supported for torch.Tensor. BICUBIC resample will be used as an alternative. Please fall back to image processor if you want full consistency with the original model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>User: The video genres are Shounen, Action, School, Super Power. The main characters are All Might, Katsuki Bakugou, Tenya Iida, Izuku Midoriya, Ochako Uraraka. Caption the video. You are provided the following series of thirty-two frames from a 0:25:06 [H:MM:SS] video.\\n\\nFrame from 00:00:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 00:43:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 01:33:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 02:23:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 03:07:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 03:57:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 04:47:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 05:37:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 06:21:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 07:11:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 08:01:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 08:51:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 09:35:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 10:25:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 11:15:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 12:05:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 12:48:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 13:38:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 14:28:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 15:18:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 16:02:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 16:52:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 17:42:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 18:32:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 19:16:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 20:06:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 20:56:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 21:46:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 22:30:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 23:20:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 24:10:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\nFrame from 25:00:<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image>\\n\\n<end_of_utterance>\\nAssistant: The appearance of \"quirks,\" newly discovered super powers, has been steadily increasing over the years, with 80 percent of humanity possessing various abilities from manipulation of elements to shapeshifting. This leaves the remainder of the world completely powerless, and Izuku Midoriya is one such individual.\\n\\nSince he was a child, the ambitious middle schooler has wanted nothing more than to be a hero. Izuku\\'s unfair fate leaves him admiring heroes and taking notes on them whenever he can. But it seems that his persistence has borne some fruit: Izuku meets the number one hero and his personal idol, All Might. All Might\\'s quirk is a unique ability that can be inherited, and he has chosen Izuku to be his successor!\\n\\nEnduring many months of grueling training, Izuku enrolls in UA High, a prestigious high school famous for its excellent hero training program, and this year\\'s freshmen look especially promising. With his bizarre but talented classmates and the looming threat of a villainous organization, Izuku will soon learn what it really means to be a hero.<end_of_utterance>\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Разделим выборку на обучающую и тестовую",
   "id": "f3e6ba9ad1b0ff52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:52:52.265672Z",
     "start_time": "2025-07-30T23:52:52.260667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds, eval_ds = random_split(\n",
    "    anime_dataset,\n",
    "    [0.95, 0.05],\n",
    "    torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Train dataset length: {len(train_ds)}; eval dataset length: {len(eval_ds)}\")"
   ],
   "id": "69bfb44b6a35be13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 584; eval dataset length: 30\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Создадим функцию по сборке набора данных во входной batch модели.\n",
    "\n",
    "При решении задачи предсказания следующего токена формировании `lables` происходит на основе входных токенов, сдвинутых на 1 позицию вперёд. Во всех примерах реализации `collate_fn` отсутствует сдвиг вперёд. Причиной этого является уже встроенная функция сдвига в loss функцию (см. реализацию `transformers.loss.loss_utils.ForCausalLMLoss`).\n",
    "\n",
    "При решении задачи инструктивного обучения по логике loss функция должна рассчитываться только от ответа ассистента. В большинстве обучающих примеров расчет происходит на всем выходе модели (за исключением специфичных токенов, таких как `<image>`) (прим. [официальная](https://github.com/huggingface/smollm/blob/main/vision/finetuning/SmolVLM2_Video_FT.ipynb) инструкция по fine-tuning SmolVLM2). Возможно, это происходит потому, что на текущий момент нет полноценно работающего инструмента внутри `transformers` для получения маски ответа ассистента. Текущая официальная реализация требует наличие маркера `{% generate %}` в шаблоне чата модели ([вопрос](https://github.com/huggingface/transformers/issues/33091) и [решение](https://github.com/huggingface/transformers/pull/30650)), что не подходит для всех моделей (Llama, Queen и сама SmolVLM от команды huggingface)."
   ],
   "id": "909397d3990cb313"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:05.153678Z",
     "start_time": "2025-07-30T22:00:05.143170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class ChatTemplateVLMCasualCollator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            processor: ProcessorMixin,\n",
    "            thread_paralleling: bool = True,\n",
    "            image_dtype=torch.float32\n",
    "    ):\n",
    "        self.processor = processor\n",
    "        self._processor_assistant_mask_available = True\n",
    "        self._thread_paralleling = thread_paralleling\n",
    "        self._image_dtype = image_dtype\n",
    "\n",
    "    def single_message_prepare(self, messages: list[dict[str, Any]]):\n",
    "        # Преобразуем сообщение чата в набор признаков\n",
    "        instance = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,  # Отключаем добавление шаблона генерации продолжения\n",
    "            tokenize=True,  # Токенизируем входной текст\n",
    "            return_dict=True,  # Возврат всех данных, а не только \"input_ids\"\n",
    "            return_assistant_tokens_mask=self._processor_assistant_mask_available,  # Возврат маски ответа ассистента\n",
    "            # padding=True,  # Добавление padding для текста\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Добавим токены выхода\n",
    "        if \"labels\" not in instance:\n",
    "            # Выход - входные токены модели (сдвиг на 1 токен внутри loss функции)\n",
    "            labels = instance[\"input_ids\"].clone()\n",
    "            # Удалим специальные токены\n",
    "            if hasattr(self.processor, \"image_token_id\"):\n",
    "                labels[labels == self.processor.image_token_id] = -100\n",
    "            # Проверим маску ассистента на некорретность\n",
    "            if (\"assistant_masks\" in instance\n",
    "                    and instance[\"assistant_masks\"].element_size() > 0\n",
    "                    and instance[\"assistant_masks\"].sum() == 0\n",
    "            ):\n",
    "                warnings.warn(f\"{processor.__class__.__name__} generate empty 'assistant_masks' output. Using assistant masked labels disabled\")\n",
    "                self._processor_assistant_mask_available = False\n",
    "            # Применим маску ассистента к выходу\n",
    "            if self._processor_assistant_mask_available:\n",
    "                labels = labels.masked_fill(~instance[\"assistant_masks\"].astype(bool), -100)\n",
    "            instance[\"labels\"] = labels\n",
    "\n",
    "        return instance\n",
    "\n",
    "    def __call__(self, examples: list[dict[str, list[dict[str, Any]]]]) -> dict[str, Any]:\n",
    "        # Ввиду того, что apply_chat_template не работает с видео разной длины - обработаем каждое сообщение по отдельности\n",
    "        if self._thread_paralleling:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                instances = list(executor.map(\n",
    "                    self.single_message_prepare,\n",
    "                    [ex['messages'] for ex in examples])\n",
    "                )\n",
    "        else:\n",
    "            instances = [\n",
    "                self.single_message_prepare(ex['messages'])\n",
    "                for ex in examples\n",
    "            ]\n",
    "        if len(instances) == 1:\n",
    "            return {\n",
    "                \"input_ids\": instances[0][\"input_ids\"],\n",
    "                \"attention_mask\": instances[0][\"attention_mask\"],\n",
    "                \"labels\": instances[0][\"labels\"],\n",
    "                \"pixel_values\": instances[0][\"pixel_values\"].to(self._image_dtype)\n",
    "            }\n",
    "\n",
    "        # Объединим данные в единые тензоры\n",
    "        out = {}\n",
    "        for field_name, pad_value in (\n",
    "                (\"input_ids\", processor.tokenizer.pad_token_id),\n",
    "                (\"attention_mask\", 0),\n",
    "                (\"labels\", -100)\n",
    "        ):\n",
    "            out[field_name] = pad_sequence(\n",
    "                [inst[field_name].squeeze(0) for inst in instances],\n",
    "                batch_first=True,\n",
    "                padding_value=pad_value\n",
    "            )\n",
    "\n",
    "        # Объединим кадры\n",
    "        # Получим требуемый общий размер объединенного тензора\n",
    "        pvs = [inst[\"pixel_values\"].squeeze(0) for inst in instances if \"pixel_values\" in inst]\n",
    "        if pvs:  # there is at least one non-None pixel_values\n",
    "            max_frames = max(pv.shape[0] for pv in pvs)\n",
    "            max_h = max(pv.shape[-2] for pv in pvs)\n",
    "            max_w = max(pv.shape[-1] for pv in pvs)\n",
    "        else:\n",
    "            max_h = max_w = processor.video_size['longest_edge']\n",
    "            max_frames = 1\n",
    "\n",
    "        padded_pixel_values = torch.zeros(\n",
    "            (len(instances), max_frames, 3, max_h, max_w),\n",
    "            dtype=self._image_dtype\n",
    "        )\n",
    "        for inst_idx, ex in enumerate(instances):\n",
    "            pv = ex.get(\"pixel_values\", None).squeeze(0)\n",
    "            # Если есть изображения в инструкции\n",
    "            if pv is not None:\n",
    "                f, _, h, w = pv.shape\n",
    "                padded_pixel_values[inst_idx, :f, :, :h, :w] = pv\n",
    "        out[\"pixel_values\"] = padded_pixel_values\n",
    "\n",
    "        return out"
   ],
   "id": "ef1c38ef8b67e4a3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:05.304353Z",
     "start_time": "2025-07-30T22:00:05.300923Z"
    }
   },
   "cell_type": "code",
   "source": "collator = ChatTemplateVLMCasualCollator(processor=processor, thread_paralleling=True, image_dtype=model.dtype)",
   "id": "d28500b8e3617ee8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверим работоспособность сборщика",
   "id": "4ab8a1fcef1ee556"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:08.264791Z",
     "start_time": "2025-07-30T22:00:05.449897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collate_data = collator([anime_dataset[i] for i in range(1)])\n",
    "collate_data.keys()"
   ],
   "id": "32ba4efc796d136d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "return_assistant_tokens_mask==True but chat template does not contain `{% generation %}` keyword.\n",
      "C:\\Users\\Domni\\AppData\\Local\\Temp\\ipykernel_29312\\1345871012.py:40: UserWarning: SmolVLMProcessor generate empty 'assistant_masks' output. Using assistant masked labels disabled\n",
      "  warnings.warn(f\"{processor.__class__.__name__} generate empty 'assistant_masks' output. Using assistant masked labels disabled\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'pixel_values'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучим модель",
   "id": "8ce7bf9596e879eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Зададим параметры обучения",
   "id": "670665e366636b8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:55:18.083383Z",
     "start_time": "2025-07-30T23:55:18.079890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_epochs = 1\n",
    "batch_size = 1\n",
    "target_batch_size = 32\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "accumulation_steps = target_batch_size // batch_size"
   ],
   "id": "fca9f292b74c48e7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:10.246856Z",
     "start_time": "2025-07-30T22:00:09.612865Z"
    }
   },
   "cell_type": "code",
   "source": "from trl import SFTTrainer, SFTConfig",
   "id": "4d617964ec9288fa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:10.448942Z",
     "start_time": "2025-07-30T22:00:10.400417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_config = SFTConfig(\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,\n",
    "    output_dir=f\"./{model_name}-anime-caption-sft\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_pin_memory=False,\n",
    "    # Ускорение обучение за счёт компиляции модели\n",
    "    # torch_compile=True,\n",
    "    # torch_compile_backend=\"inductor\",\n",
    "    # torch_compile_mode=\"default\",\n",
    "    gradient_checkpointing=True,  # Leads to reduction in memory at slighly decrease in speed\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # Set gradient checkpointing to non-reentrant to avoid issues.\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}\n",
    ")"
   ],
   "id": "39f92b3c3966e169",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T22:00:10.759967Z",
     "start_time": "2025-07-30T22:00:10.598831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_ds,\n",
    "    processing_class=processor,\n",
    ")"
   ],
   "id": "9055e4085794fd65",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:05:15.021243Z",
     "start_time": "2025-07-30T22:00:10.959311Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "146c4c1cfae1400d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 1:02:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19, training_loss=4.2534998743740433e+30, metrics={'train_runtime': 3903.7148, 'train_samples_per_second': 0.15, 'train_steps_per_second': 0.005, 'total_flos': 4479925849303296.0, 'train_loss': 4.2534998743740433e+30})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Посмотрим на пример генерации",
   "id": "3a1c26725c3dfe1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Напишем простенькую функцию для генерации ответа",
   "id": "8a931c904d56d373"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:01:31.724915Z",
     "start_time": "2025-07-31T00:01:31.720423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_caption(\n",
    "        model,\n",
    "        processor,\n",
    "        conversation\n",
    "):\n",
    "    model.eval()\n",
    "    message = conversation[\"messages\"]\n",
    "    # Уберём ответ агента, если он присутствует\n",
    "    if message[-1][\"role\"] == \"assistant\":\n",
    "        message = message[:-1]\n",
    "\n",
    "    instance = processor.apply_chat_template(\n",
    "        message,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device).to(model.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**instance, do_sample=False, max_new_tokens=256)\n",
    "    # print(instance[\"input_ids\"].size())\n",
    "    # print(generated_ids.size())\n",
    "    # print(generated_ids[len(instance[\"input_ids\"]):])\n",
    "\n",
    "    generated_texts = processor.batch_decode(\n",
    "        generated_ids[0][len(instance[\"input_ids\"][0]):],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    generated_texts = \"\".join(generated_texts)\n",
    "\n",
    "    return generated_texts"
   ],
   "id": "fc39745405d25dd",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузим исходную версию модели, чтобы сравнить качество генерации",
   "id": "512487bf7b8d9927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:53:25.128201Z",
     "start_time": "2025-07-30T23:53:18.362993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vanila_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    "    device_map=device,\n",
    ")"
   ],
   "id": "f9b4a0aaff9b8aca",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузим дообученную модель",
   "id": "1b46619e1eb6a5c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:55:30.662040Z",
     "start_time": "2025-07-30T23:55:27.501630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    f\"./{model_name}-anime-caption-sft/checkpoint-19\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    "    device_map=device,\n",
    ")"
   ],
   "id": "82a3695d6c6041a4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проведём сравнение",
   "id": "d005733d644a5cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:02:40.804965Z",
     "start_time": "2025-07-31T00:01:36.734952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generate_test_message = eval_ds[1]\n",
    "generate_test_anime_data = anime_dataset.get_anime_data_by_idx(eval_ds.indices[1])\n",
    "print(\n",
    "    f\"\\tTitle: {generate_test_anime_data.title}\\n\"\n",
    "    f\"\\tTrue Caption:\\n{generate_test_anime_data.description}\\n\\n\"\n",
    "    f\"-------------------------------\\n\\n\"\n",
    ")\n",
    "\n",
    "vanila_model_test_generation = generate_caption(vanila_model, processor, generate_test_message)\n",
    "print(\n",
    "    f\"\\tVanila model generation:\\n{vanila_model_test_generation}\\n\\n\"\n",
    "    f\"-------------------------------\\n\\n\"\n",
    ")\n",
    "\n",
    "trained_model_test_generation = generate_caption(trained_model, processor, generate_test_message)\n",
    "print(\n",
    "    f\"\\tTrained model generation:\\n{trained_model_test_generation}\\n\\n\"\n",
    "    f\"-------------------------------\\n\\n\"\n",
    ")"
   ],
   "id": "b38a70f4313a2c8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTitle: Mushibugyo\n",
      "\tTrue Caption:\n",
      "A menace of huge monster-like insects is plaguing the land of Edo. Too powerful to be subdued by ordinary folks, the creatures are hunted by the Insect Magistrates—a group of warriors who specialize in various secret arts and combat styles. To bolster their strength, they summon Genjuurou Tsukishima, the master swordsman from the Tsugaru Province.\n",
      "\n",
      "Due to an unfortunate incident, however, Genjuurou is incapable of answering the call and sends his son, Jinbee, to serve in his stead. Determined to atone for the incident caused by his own cowardice, Jinbee agrees to travel to Edo and join the Insect Magistrates. Armed with his fiery spirit and unwavering resolve, Jinbee vows to become a stronger samurai and rid Edo of the insect threat once and for all.\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "\n",
      "\tVanila model generation:\n",
      " The video features a series of scenes from an anime, each with distinct characters and settings. The first scene shows a character in a traditional Japanese setting, possibly a samurai, with a sword and a helmet. The background is a serene landscape with a mountain and a body of water. The second scene features a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object in the center. The third scene shows a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object in the center. The fourth scene features a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object in the center. The fifth scene shows a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object in the center. The sixth scene features a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object in the center. The seventh scene shows a character in a dark, gothic-style room, possibly a temple or a dungeon, with a large, glowing object\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "\n",
      "\tTrained model generation:\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Вывод\n",
    "\n",
    "По результатам проделанной работы можно выделить следующее:\n",
    "- Исходная модель SmolVLM2 хорошо справляется с описанием каждого кадра. Возможно, если провести prompt-engineering, то можно добиться хорошего результата по обобщению информации с кадров в одно цельное описание видео\n",
    "- Обработка длинных видео даже достаточно малыми моделями является требовательной операцией по памяти графического процессора, т.к. обработка одного обучающего примера из 32 изображений моделью на 500 млн параметров потребовало 12 GB памяти CPU.\n",
    "- Обучение на 1 эпохе занимает 1 час на RTX 4060 - основное ограничение идёт по памяти, из за чего CPU простаивает 99% времени\n",
    "- После слишком короткого дообучения модель перестала справляться с генерацией предложений, что может быть вызвано слишком малым количеством пройденных эпох.\n",
    "- Хоть провести полноценное обучение не хватило технических ресурсов, в рамках тетрадки был реализован полный pipeline дообучения на собственном наборе данных"
   ],
   "id": "d473d1a40017454e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extra: Нереализованные идеи",
   "id": "f7a7918606a892c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Добавление поддержки assistant_mask в SmolVLM2",
   "id": "c2a59fbf859a08a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Добавим поддержку с получением маски ассистента ([вопрос](https://github.com/huggingface/transformers/issues/33091) и [решение](https://github.com/huggingface/transformers/pull/30650).\n",
    "\n",
    "Для этого внедрим метки `{% generation %}` и `{% endgeneration %}` в шаблон сообщения ассистента.\n",
    "\n",
    "Если в скором времени данную проблему решат более правильным способом - уберите этот блок"
   ],
   "id": "7709c11eff17c333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:05:15.402601Z",
     "start_time": "2025-07-30T23:05:15.397088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# target_template = \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n",
    "#\n",
    "# if processor.chat_template == target_template:\n",
    "#     processor.chat_template = (\n",
    "#         \"<|im_start|>\"\n",
    "#         \"{% for message in messages %}\"\n",
    "#         \"{{message['role'] | capitalize}}\"\n",
    "#         \"{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}\"\n",
    "#         \"{% if message['role'] | lower != 'assistant' %}\"\n",
    "#         \"{% for line in message['content'] %}\"\n",
    "#         \"{% if line['type'] == 'text' %}{{line['text']}}\"\n",
    "#         \"{% elif line['type'] == 'image' %}{{ '<image>' }}\"\n",
    "#         \"{% endif %}\"\n",
    "#         \"{% endfor %}\"\n",
    "#         \"{% else %}\"\n",
    "#         \"{% generation %}\"  # Add generation marker\n",
    "#         \"{% for line in message['content'] %}{{line['text']}}{% endfor %}\"\n",
    "#         \"{% endgeneration %}\"  # Add endgeneration marker\n",
    "#         \"{% endif %}\"\n",
    "#         \"<end_of_utterance>\\n\"\n",
    "#         \"{% endfor %}\"\n",
    "#         \"{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n",
    "#     )\n",
    "# else:\n",
    "#     warnings.warn('This resolve only applied for SmolVLMProcessor. For other model change template manually')"
   ],
   "id": "f8cd10b3a02cbb54",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверим, что генерация вспомогательной маски работает корректно",
   "id": "63eff1bbdbde9fa0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T23:05:15.412214Z",
     "start_time": "2025-07-30T23:05:15.407880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_assistant_mask_message = [\n",
    "#     {\"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"Caption the video.\"},\n",
    "#         # {\"type\": \"video\", \"path\": \"https://huggingface.co/datasets/hexuan21/VideoFeedback-videos-mp4/resolve/main/p/p110924.mp4\"}\n",
    "#     ]},\n",
    "#     {\"role\": \"assistant\", \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"A dog inside of a dog kennel on a patio.\"},\n",
    "#     ]}\n",
    "# ]\n",
    "# print(processor.apply_chat_template(\n",
    "#             test_assistant_mask_message,\n",
    "#             add_generation_prompt=False,\n",
    "#             tokenize=False,\n",
    "#         ))\n",
    "# test_assistant_mask_instance = processor.apply_chat_template(\n",
    "#     test_assistant_mask_message,\n",
    "#     add_generation_prompt=False,\n",
    "#     tokenize=True,\n",
    "#     return_dict=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_assistant_tokens_mask=True,\n",
    "# )\n",
    "# assert test_assistant_mask_instance['assistant_masks'].sum() != 0\n",
    "# del test_assistant_mask_message, test_assistant_mask_instance"
   ],
   "id": "8a760b9afebe6fef",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Вывод: на текущей версии `transformers` даже после улучшений возвращается пустая маска",
   "id": "baa8bd49a54a462d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ea26ebc9c2268b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
